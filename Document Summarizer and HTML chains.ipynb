{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7585270c",
   "metadata": {},
   "source": [
    "# Utility Chains Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fc6f5",
   "metadata": {},
   "source": [
    "## Summarizing Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782af7d",
   "metadata": {},
   "source": [
    "### load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c27204db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai==1.14.2\n",
      "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai==1.14.2) (2.8.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai==1.14.2) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai==1.14.2) (4.12.2)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai==1.14.2) (4.62.3)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai==1.14.2) (0.27.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai==1.14.2) (4.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai==1.14.2) (1.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.14.2) (1.1.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai==1.14.2) (3.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.14.2) (1.0.5)\n",
      "Requirement already satisfied: certifi in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai==1.14.2) (2021.10.8)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.14.2) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.14.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai==1.14.2) (2.20.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from tqdm>4->openai==1.14.2) (0.4.4)\n",
      "Installing collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.10.0\n",
      "    Uninstalling openai-1.10.0:\n",
      "      Successfully uninstalled openai-1.10.0\n",
      "Successfully installed openai-1.14.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.13\n",
      "  Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (2.8.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (6.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (4.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (3.10.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (1.4.22)\n",
      "Collecting langchain-core<0.2.0,>=0.1.33\n",
      "  Using cached langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (2.26.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.29\n",
      "  Using cached langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (0.6.7)\n",
      "Collecting langchain-text-splitters<0.1,>=0.0.1\n",
      "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (8.5.0)\n",
      "Collecting langsmith<0.2.0,>=0.1.17\n",
      "  Downloading langsmith-0.1.120-py3-none-any.whl (289 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain==0.1.13) (1.20.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (2.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (21.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (6.0.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.13) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.13) (3.0.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain==0.1.13) (23.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (3.10.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (2021.10.8)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (3.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (1.2.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (4.4.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.13) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.13) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain==0.1.13) (0.7.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.13) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain==0.1.13) (2.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.13) (1.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.13) (0.4.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.1.13) (1.1.1)\n",
      "Installing collected packages: langsmith, langchain-core, langchain-text-splitters, langchain-community, langchain\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.0.87\n",
      "    Uninstalling langsmith-0.0.87:\n",
      "      Successfully uninstalled langsmith-0.0.87\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.23\n",
      "    Uninstalling langchain-core-0.1.23:\n",
      "      Successfully uninstalled langchain-core-0.1.23\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.2.2\n",
      "    Uninstalling langchain-text-splitters-0.2.2:\n",
      "      Successfully uninstalled langchain-text-splitters-0.2.2\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.20\n",
      "    Uninstalling langchain-community-0.0.20:\n",
      "      Successfully uninstalled langchain-community-0.0.20\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.4\n",
      "    Uninstalling langchain-0.1.4:\n",
      "      Successfully uninstalled langchain-0.1.4\n",
      "Successfully installed langchain-0.1.13 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 langsmith-0.1.120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-google-genai 1.0.6 requires langchain-core<0.3,>=0.2.2, but you have langchain-core 0.1.52 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface-hub==0.21.4\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from huggingface-hub==0.21.4) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from huggingface-hub==0.21.4) (4.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from huggingface-hub==0.21.4) (2.26.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from huggingface-hub==0.21.4) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from huggingface-hub==0.21.4) (6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from huggingface-hub==0.21.4) (23.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from huggingface-hub==0.21.4) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub==0.21.4) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests->huggingface-hub==0.21.4) (1.26.7)\n",
      "Installing collected packages: huggingface-hub\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.23.4\n",
      "    Uninstalling huggingface-hub-0.23.4:\n",
      "      Successfully uninstalled huggingface-hub-0.23.4\n",
      "Successfully installed huggingface-hub-0.21.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.44.2 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.21.4 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai==0.1.0\n",
      "  Downloading langchain_openai-0.1.0-py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-openai==0.1.0) (0.5.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-openai==0.1.0) (1.14.2)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-openai==0.1.0) (0.1.52)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (8.5.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (23.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (6.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.1.120)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.33)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (3.10.7)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.27.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.26.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.0.5)\n",
      "Requirement already satisfied: anyio in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2021.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (3.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.14.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (1.9.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (4.62.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.1.1)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.26.7)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.0) (2024.7.24)\n",
      "Requirement already satisfied: colorama in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from tqdm>4->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (0.4.4)\n",
      "Installing collected packages: langchain-openai\n",
      "  Attempting uninstall: langchain-openai\n",
      "    Found existing installation: langchain-openai 0.0.5\n",
      "    Uninstalling langchain-openai-0.0.5:\n",
      "      Successfully uninstalled langchain-openai-0.0.5\n",
      "Successfully installed langchain-openai-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: The candidate selected for download or install is a yanked version: 'langchain-openai' candidate (version 0.1.0 at https://files.pythonhosted.org/packages/6d/f4/bea64066e93a4980f0d8352af733f950ff0eea98cca5000a4ca1ff2ae2b8/langchain_openai-0.1.0-py3-none-any.whl#sha256=3cb3da679da9bc33c20634a5a64e65e1f1d9f69a8b71e8622297d8c33623e8d6 (from https://pypi.org/simple/langchain-openai/) (requires-python:<4.0,>=3.8.1))\n",
      "Reason for being yanked: Contained a regression that prevented passing ToolMessage in the input to ChatOpenAI, fixed in 0.1.1\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.5.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from tiktoken==0.5.2) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from tiktoken==0.5.2) (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.5.2) (1.26.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4==0.0.2\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from bs4==0.0.2) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sharv\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4==0.0.2) (2.2.1)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\sharv\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai==1.14.2\n",
    "!pip install langchain==0.1.13\n",
    "!pip install huggingface-hub==0.21.4\n",
    "!pip install langchain-openai==0.1.0\n",
    "!pip install tiktoken==0.5.2\n",
    "!pip install bs4==0.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db3f812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb56eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333eca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "369ec2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the document\n",
    "with open(\"sample.txt\", encoding = \"utf8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a7a77ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deep Clean Your Data: Advanced Techniques for the Clever Analyst\\nEver feel like your data analysis is looking through a fog? Inconsistent formatting, typos, and missing information can all cloud the picture, leading to misleading results. Data cleaning is the antidote that clarifies and sharpens your data for trustworthy analysis. By tackling these imperfections, you’ll gain a clear view to make sound decisions and avoid costly missteps. This article dives into the essential data cleaning methods, helping you transform your data from murky to magnificent.\\n\\n⌘ Handling Missing Values in Data\\nEncountering missing data points can be a hurdle in data analysis. Removing rows or columns with these blanks may seem like a quick fix, but it can discard valuable information. Thankfully, there are other options! Data imputation allows you to estimate missing values. This can involve replacing them with averages, using sophisticated algorithms, or even creating a special category for “missing” data itself. The most effective method depends on the type of data you’re working with (numbers or categories) and the reason the data is missing in the first place.\\n\\n⌘\\xa0Spotting and Managing Outliers\\nOutliers, data points that stray far from the pack, can wreak havoc on your analysis. To identify them, keep an eye on visualizations like boxplots. Once spotted, you have choices. Clear errors can be removed, but be mindful as outliers can sometimes reveal hidden truths! Consider investigating the cause, it might expose a fascinating trend. Alternatively, you can replace the outlier with a more fitting value, like the median. The best course of action depends on your specific data and analysis goals.\\n\\n⌘\\xa0Eliminating Duplicates\\nDuplicate data can make your analysis a messy affair. Fortunately, eliminating them is a straightforward process. Most spreadsheet programs offer a “remove duplicates” function. This handy tool identifies and removes exact copies within your chosen data set, streamlining your analysis. However, be aware that slight variations in duplicates might slip through the cracks. To ensure a truly clean dataset, you may need to specify which columns to compare for complete elimination.\\n\\n⌘\\xa0Transforming Texts for Clarity\\nText data cleaning often involves wrestling with inconsistencies and unwanted characters. Here’s where regular expressions (regex) come in as your secret weapon. Regex uses patterns to identify and manipulate text. You can write a regex pattern to target things like extra spaces, punctuation, special symbols, or even hashtags. Once you’ve identified the pattern, you can replace it with a clean alternative, like an empty space or complete removal. This wrangles your text data into a uniform format, making it easier to analyze and interpret for better results.\\n\\n⌘\\xa0Taming Categorical Variables\\nFor many machine learning tasks, categorical variables need to be transformed into numerical values. One approach is encoding, where we simply assign a unique number to each category. However, for some situations, we might want to encode in a way that reflects the order or relationship between the categories. This can be achieved through techniques like one-hot encoding, which creates a new binary variable for each category.\\n\\n⌘\\xa0Date & Time Mastery\\nData from different sources might use YYYY-MM-DD, MM/DD/YYYY, or even cryptic abbreviations. Standardization tools can convert everything to a single, consistent format. Advanced cleaning might involve handling partial dates (like just a year) or timestamps with time zones. Here, we might need to decide on a default date or time zone to ensure consistency. Finally, some analyses might require extracting specific parts from the date and time data, like the day of the week or hour.\\n\\n⌘ Solving Imbalanced Data Distribution\\nMore than standard techniques might be needed for highly imbalanced data, where one class vastly outnumbers the others. Here’s where advanced approaches come in. Resampling methods like SMOTE (Synthetic Minority Oversampling Technique) can create synthetic data points for the minority class. Imagine generating new, realistic data examples to balance the scales! Additionally, specialized algorithms are designed specifically for imbalanced data. These algorithms can place more weight on the minority class during training, ensuring the model pays closer attention to the rarer but crucial examples.\\n\\n⌘ Data\\xa0Scaling for Calibration\\nFeatures in your data can have uneven scales. Data scaling fixes this! Normalization transforms all features to have a mean of zero and a standard deviation of one, ensuring each feature contributes equally. Min-max scaling, on the other hand, rescales features to a specific range, often 0 to 1. By applying these scaling techniques, you create a level playing field for all features in your data.\\n\\n⌘ Adjusting for Data Skewness\\nEncountering skewed data, where values favor one extreme, can throw your analysis off balance. Imagine analyzing income with a ruler that only shows low numbers! To tackle this, data transformations come in handy. A popular technique is the log transformation, which acts like a data shrink ray. It compresses large values, making them less influential and giving smaller values a fairer chance.\\n\\n⌘\\xa0Mapping the Data Journey\\nData lineage tracks the entire journey of a data point, like a detailed travel log. It shows where the data came from, all its transformations, and its final use in analysis or reports. This transparency helps identify bottlenecks, ensure data quality, and meet regulations.\\nData provenance, on the other hand, focuses on the data’s origin story. It’s like a birth certificate, establishing the source and any inherent qualities the data might have. Knowing the source helps assess its credibility and allows for reproducing analyses or debugging errors. By tracking lineage and provenance, organizations can ensure data quality and facilitate debugging if issues arise.\\n\\nConclusion\\nI hope you enjoyed reading this article on important data-cleaning concepts we should follow as a checklist. If you would like to connect with me to discuss this topic further, please reach out via LinkedIn or email me directly. I’m always happy to answer any additional questions you may have on statistics or other data science techniques.\\nThank you for reading, and I look forward to hearing from you!\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c86c6b",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "When it comes to document processing, breaking a large document into smaller, more manageable chunks is essential\n",
    "<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e378c705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text\n",
    "text_splitter = CharacterTextSplitter()\n",
    "texts = text_splitter.split_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0393c2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep Clean Your Data: Advanced Techniques for the Clever Analyst\\nEver feel like your data analysis is looking through a fog? Inconsistent formatting, typos, and missing information can all cloud the picture, leading to misleading results. Data cleaning is the antidote that clarifies and sharpens your data for trustworthy analysis. By tackling these imperfections, you’ll gain a clear view to make sound decisions and avoid costly missteps. This article dives into the essential data cleaning methods, helping you transform your data from murky to magnificent.\\n\\n⌘ Handling Missing Values in Data\\nEncountering missing data points can be a hurdle in data analysis. Removing rows or columns with these blanks may seem like a quick fix, but it can discard valuable information. Thankfully, there are other options! Data imputation allows you to estimate missing values. This can involve replacing them with averages, using sophisticated algorithms, or even creating a special category for “missing” data itself. The most effective method depends on the type of data you’re working with (numbers or categories) and the reason the data is missing in the first place.\\n\\n⌘\\xa0Spotting and Managing Outliers\\nOutliers, data points that stray far from the pack, can wreak havoc on your analysis. To identify them, keep an eye on visualizations like boxplots. Once spotted, you have choices. Clear errors can be removed, but be mindful as outliers can sometimes reveal hidden truths! Consider investigating the cause, it might expose a fascinating trend. Alternatively, you can replace the outlier with a more fitting value, like the median. The best course of action depends on your specific data and analysis goals.\\n\\n⌘\\xa0Eliminating Duplicates\\nDuplicate data can make your analysis a messy affair. Fortunately, eliminating them is a straightforward process. Most spreadsheet programs offer a “remove duplicates” function. This handy tool identifies and removes exact copies within your chosen data set, streamlining your analysis. However, be aware that slight variations in duplicates might slip through the cracks. To ensure a truly clean dataset, you may need to specify which columns to compare for complete elimination.\\n\\n⌘\\xa0Transforming Texts for Clarity\\nText data cleaning often involves wrestling with inconsistencies and unwanted characters. Here’s where regular expressions (regex) come in as your secret weapon. Regex uses patterns to identify and manipulate text. You can write a regex pattern to target things like extra spaces, punctuation, special symbols, or even hashtags. Once you’ve identified the pattern, you can replace it with a clean alternative, like an empty space or complete removal. This wrangles your text data into a uniform format, making it easier to analyze and interpret for better results.\\n\\n⌘\\xa0Taming Categorical Variables\\nFor many machine learning tasks, categorical variables need to be transformed into numerical values. One approach is encoding, where we simply assign a unique number to each category. However, for some situations, we might want to encode in a way that reflects the order or relationship between the categories. This can be achieved through techniques like one-hot encoding, which creates a new binary variable for each category.\\n\\n⌘\\xa0Date & Time Mastery\\nData from different sources might use YYYY-MM-DD, MM/DD/YYYY, or even cryptic abbreviations. Standardization tools can convert everything to a single, consistent format. Advanced cleaning might involve handling partial dates (like just a year) or timestamps with time zones. Here, we might need to decide on a default date or time zone to ensure consistency. Finally, some analyses might require extracting specific parts from the date and time data, like the day of the week or hour.',\n",
       " '⌘ Solving Imbalanced Data Distribution\\nMore than standard techniques might be needed for highly imbalanced data, where one class vastly outnumbers the others. Here’s where advanced approaches come in. Resampling methods like SMOTE (Synthetic Minority Oversampling Technique) can create synthetic data points for the minority class. Imagine generating new, realistic data examples to balance the scales! Additionally, specialized algorithms are designed specifically for imbalanced data. These algorithms can place more weight on the minority class during training, ensuring the model pays closer attention to the rarer but crucial examples.\\n\\n⌘ Data\\xa0Scaling for Calibration\\nFeatures in your data can have uneven scales. Data scaling fixes this! Normalization transforms all features to have a mean of zero and a standard deviation of one, ensuring each feature contributes equally. Min-max scaling, on the other hand, rescales features to a specific range, often 0 to 1. By applying these scaling techniques, you create a level playing field for all features in your data.\\n\\n⌘ Adjusting for Data Skewness\\nEncountering skewed data, where values favor one extreme, can throw your analysis off balance. Imagine analyzing income with a ruler that only shows low numbers! To tackle this, data transformations come in handy. A popular technique is the log transformation, which acts like a data shrink ray. It compresses large values, making them less influential and giving smaller values a fairer chance.\\n\\n⌘\\xa0Mapping the Data Journey\\nData lineage tracks the entire journey of a data point, like a detailed travel log. It shows where the data came from, all its transformations, and its final use in analysis or reports. This transparency helps identify bottlenecks, ensure data quality, and meet regulations.\\nData provenance, on the other hand, focuses on the data’s origin story. It’s like a birth certificate, establishing the source and any inherent qualities the data might have. Knowing the source helps assess its credibility and allows for reproducing analyses or debugging errors. By tracking lineage and provenance, organizations can ensure data quality and facilitate debugging if issues arise.\\n\\nConclusion\\nI hope you enjoyed reading this article on important data-cleaning concepts we should follow as a checklist. If you would like to connect with me to discuss this topic further, please reach out via LinkedIn or email me directly. I’m always happy to answer any additional questions you may have on statistics or other data science techniques.\\nThank you for reading, and I look forward to hearing from you!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e33b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple documents\n",
    "docs = [Document(page_content=t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7b6158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Deep Clean Your Data: Advanced Techniques for the Clever Analyst\\nEver feel like your data analysis is looking through a fog? Inconsistent formatting, typos, and missing information can all cloud the picture, leading to misleading results. Data cleaning is the antidote that clarifies and sharpens your data for trustworthy analysis. By tackling these imperfections, you’ll gain a clear view to make sound decisions and avoid costly missteps. This article dives into the essential data cleaning methods, helping you transform your data from murky to magnificent.\\n\\n⌘ Handling Missing Values in Data\\nEncountering missing data points can be a hurdle in data analysis. Removing rows or columns with these blanks may seem like a quick fix, but it can discard valuable information. Thankfully, there are other options! Data imputation allows you to estimate missing values. This can involve replacing them with averages, using sophisticated algorithms, or even creating a special category for “missing” data itself. The most effective method depends on the type of data you’re working with (numbers or categories) and the reason the data is missing in the first place.\\n\\n⌘\\xa0Spotting and Managing Outliers\\nOutliers, data points that stray far from the pack, can wreak havoc on your analysis. To identify them, keep an eye on visualizations like boxplots. Once spotted, you have choices. Clear errors can be removed, but be mindful as outliers can sometimes reveal hidden truths! Consider investigating the cause, it might expose a fascinating trend. Alternatively, you can replace the outlier with a more fitting value, like the median. The best course of action depends on your specific data and analysis goals.\\n\\n⌘\\xa0Eliminating Duplicates\\nDuplicate data can make your analysis a messy affair. Fortunately, eliminating them is a straightforward process. Most spreadsheet programs offer a “remove duplicates” function. This handy tool identifies and removes exact copies within your chosen data set, streamlining your analysis. However, be aware that slight variations in duplicates might slip through the cracks. To ensure a truly clean dataset, you may need to specify which columns to compare for complete elimination.\\n\\n⌘\\xa0Transforming Texts for Clarity\\nText data cleaning often involves wrestling with inconsistencies and unwanted characters. Here’s where regular expressions (regex) come in as your secret weapon. Regex uses patterns to identify and manipulate text. You can write a regex pattern to target things like extra spaces, punctuation, special symbols, or even hashtags. Once you’ve identified the pattern, you can replace it with a clean alternative, like an empty space or complete removal. This wrangles your text data into a uniform format, making it easier to analyze and interpret for better results.\\n\\n⌘\\xa0Taming Categorical Variables\\nFor many machine learning tasks, categorical variables need to be transformed into numerical values. One approach is encoding, where we simply assign a unique number to each category. However, for some situations, we might want to encode in a way that reflects the order or relationship between the categories. This can be achieved through techniques like one-hot encoding, which creates a new binary variable for each category.\\n\\n⌘\\xa0Date & Time Mastery\\nData from different sources might use YYYY-MM-DD, MM/DD/YYYY, or even cryptic abbreviations. Standardization tools can convert everything to a single, consistent format. Advanced cleaning might involve handling partial dates (like just a year) or timestamps with time zones. Here, we might need to decide on a default date or time zone to ensure consistency. Finally, some analyses might require extracting specific parts from the date and time data, like the day of the week or hour.'),\n",
       " Document(page_content='⌘ Solving Imbalanced Data Distribution\\nMore than standard techniques might be needed for highly imbalanced data, where one class vastly outnumbers the others. Here’s where advanced approaches come in. Resampling methods like SMOTE (Synthetic Minority Oversampling Technique) can create synthetic data points for the minority class. Imagine generating new, realistic data examples to balance the scales! Additionally, specialized algorithms are designed specifically for imbalanced data. These algorithms can place more weight on the minority class during training, ensuring the model pays closer attention to the rarer but crucial examples.\\n\\n⌘ Data\\xa0Scaling for Calibration\\nFeatures in your data can have uneven scales. Data scaling fixes this! Normalization transforms all features to have a mean of zero and a standard deviation of one, ensuring each feature contributes equally. Min-max scaling, on the other hand, rescales features to a specific range, often 0 to 1. By applying these scaling techniques, you create a level playing field for all features in your data.\\n\\n⌘ Adjusting for Data Skewness\\nEncountering skewed data, where values favor one extreme, can throw your analysis off balance. Imagine analyzing income with a ruler that only shows low numbers! To tackle this, data transformations come in handy. A popular technique is the log transformation, which acts like a data shrink ray. It compresses large values, making them less influential and giving smaller values a fairer chance.\\n\\n⌘\\xa0Mapping the Data Journey\\nData lineage tracks the entire journey of a data point, like a detailed travel log. It shows where the data came from, all its transformations, and its final use in analysis or reports. This transparency helps identify bottlenecks, ensure data quality, and meet regulations.\\nData provenance, on the other hand, focuses on the data’s origin story. It’s like a birth certificate, establishing the source and any inherent qualities the data might have. Knowing the source helps assess its credibility and allows for reproducing analyses or debugging errors. By tracking lineage and provenance, organizations can ensure data quality and facilitate debugging if issues arise.\\n\\nConclusion\\nI hope you enjoyed reading this article on important data-cleaning concepts we should follow as a checklist. If you would like to connect with me to discuss this topic further, please reach out via LinkedIn or email me directly. I’m always happy to answer any additional questions you may have on statistics or other data science techniques.\\nThank you for reading, and I look forward to hearing from you!')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf24b6b",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "To create an instance of load_summarizer_chain, we need to provide three arguments. <br><br>Firstly, we need to pass the desired large language model that will be used to query the user input. Secondly, we specify the type of langchain chain to be used for summarizing documents.<br> Lastly, we can set the verbose argument to True if we want to see all the intermediate steps involved in processing the user request and generating the output.<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a582191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Deep Clean Your Data: Advanced Techniques for the Clever Analyst\n",
      "Ever feel like your data analysis is looking through a fog? Inconsistent formatting, typos, and missing information can all cloud the picture, leading to misleading results. Data cleaning is the antidote that clarifies and sharpens your data for trustworthy analysis. By tackling these imperfections, you’ll gain a clear view to make sound decisions and avoid costly missteps. This article dives into the essential data cleaning methods, helping you transform your data from murky to magnificent.\n",
      "\n",
      "⌘ Handling Missing Values in Data\n",
      "Encountering missing data points can be a hurdle in data analysis. Removing rows or columns with these blanks may seem like a quick fix, but it can discard valuable information. Thankfully, there are other options! Data imputation allows you to estimate missing values. This can involve replacing them with averages, using sophisticated algorithms, or even creating a special category for “missing” data itself. The most effective method depends on the type of data you’re working with (numbers or categories) and the reason the data is missing in the first place.\n",
      "\n",
      "⌘ Spotting and Managing Outliers\n",
      "Outliers, data points that stray far from the pack, can wreak havoc on your analysis. To identify them, keep an eye on visualizations like boxplots. Once spotted, you have choices. Clear errors can be removed, but be mindful as outliers can sometimes reveal hidden truths! Consider investigating the cause, it might expose a fascinating trend. Alternatively, you can replace the outlier with a more fitting value, like the median. The best course of action depends on your specific data and analysis goals.\n",
      "\n",
      "⌘ Eliminating Duplicates\n",
      "Duplicate data can make your analysis a messy affair. Fortunately, eliminating them is a straightforward process. Most spreadsheet programs offer a “remove duplicates” function. This handy tool identifies and removes exact copies within your chosen data set, streamlining your analysis. However, be aware that slight variations in duplicates might slip through the cracks. To ensure a truly clean dataset, you may need to specify which columns to compare for complete elimination.\n",
      "\n",
      "⌘ Transforming Texts for Clarity\n",
      "Text data cleaning often involves wrestling with inconsistencies and unwanted characters. Here’s where regular expressions (regex) come in as your secret weapon. Regex uses patterns to identify and manipulate text. You can write a regex pattern to target things like extra spaces, punctuation, special symbols, or even hashtags. Once you’ve identified the pattern, you can replace it with a clean alternative, like an empty space or complete removal. This wrangles your text data into a uniform format, making it easier to analyze and interpret for better results.\n",
      "\n",
      "⌘ Taming Categorical Variables\n",
      "For many machine learning tasks, categorical variables need to be transformed into numerical values. One approach is encoding, where we simply assign a unique number to each category. However, for some situations, we might want to encode in a way that reflects the order or relationship between the categories. This can be achieved through techniques like one-hot encoding, which creates a new binary variable for each category.\n",
      "\n",
      "⌘ Date & Time Mastery\n",
      "Data from different sources might use YYYY-MM-DD, MM/DD/YYYY, or even cryptic abbreviations. Standardization tools can convert everything to a single, consistent format. Advanced cleaning might involve handling partial dates (like just a year) or timestamps with time zones. Here, we might need to decide on a default date or time zone to ensure consistency. Finally, some analyses might require extracting specific parts from the date and time data, like the day of the week or hour.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"⌘ Solving Imbalanced Data Distribution\n",
      "More than standard techniques might be needed for highly imbalanced data, where one class vastly outnumbers the others. Here’s where advanced approaches come in. Resampling methods like SMOTE (Synthetic Minority Oversampling Technique) can create synthetic data points for the minority class. Imagine generating new, realistic data examples to balance the scales! Additionally, specialized algorithms are designed specifically for imbalanced data. These algorithms can place more weight on the minority class during training, ensuring the model pays closer attention to the rarer but crucial examples.\n",
      "\n",
      "⌘ Data Scaling for Calibration\n",
      "Features in your data can have uneven scales. Data scaling fixes this! Normalization transforms all features to have a mean of zero and a standard deviation of one, ensuring each feature contributes equally. Min-max scaling, on the other hand, rescales features to a specific range, often 0 to 1. By applying these scaling techniques, you create a level playing field for all features in your data.\n",
      "\n",
      "⌘ Adjusting for Data Skewness\n",
      "Encountering skewed data, where values favor one extreme, can throw your analysis off balance. Imagine analyzing income with a ruler that only shows low numbers! To tackle this, data transformations come in handy. A popular technique is the log transformation, which acts like a data shrink ray. It compresses large values, making them less influential and giving smaller values a fairer chance.\n",
      "\n",
      "⌘ Mapping the Data Journey\n",
      "Data lineage tracks the entire journey of a data point, like a detailed travel log. It shows where the data came from, all its transformations, and its final use in analysis or reports. This transparency helps identify bottlenecks, ensure data quality, and meet regulations.\n",
      "Data provenance, on the other hand, focuses on the data’s origin story. It’s like a birth certificate, establishing the source and any inherent qualities the data might have. Knowing the source helps assess its credibility and allows for reproducing analyses or debugging errors. By tracking lineage and provenance, organizations can ensure data quality and facilitate debugging if issues arise.\n",
      "\n",
      "Conclusion\n",
      "I hope you enjoyed reading this article on important data-cleaning concepts we should follow as a checklist. If you would like to connect with me to discuss this topic further, please reach out via LinkedIn or email me directly. I’m always happy to answer any additional questions you may have on statistics or other data science techniques.\n",
      "Thank you for reading, and I look forward to hearing from you!\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" This article discusses the importance of data cleaning for accurate analysis and outlines advanced techniques for handling missing values, outliers, duplicates, and transforming text and categorical data. It also covers methods for taming date and time data for consistency and extracting specific information.\n",
      "\n",
      "\n",
      "\n",
      "This article discusses important concepts for handling imbalanced data distribution, data scaling, data skewness, and data lineage and provenance. Techniques such as resampling, scaling, and transformations can help address imbalances and ensure data quality. Tracking the journey and origin of data is crucial for identifying bottlenecks, ensuring credibility, and debugging errors. The author invites further discussion on these topics via LinkedIn or email.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='Deep Clean Your Data: Advanced Techniques for the Clever Analyst\\nEver feel like your data analysis is looking through a fog? Inconsistent formatting, typos, and missing information can all cloud the picture, leading to misleading results. Data cleaning is the antidote that clarifies and sharpens your data for trustworthy analysis. By tackling these imperfections, you’ll gain a clear view to make sound decisions and avoid costly missteps. This article dives into the essential data cleaning methods, helping you transform your data from murky to magnificent.\\n\\n⌘ Handling Missing Values in Data\\nEncountering missing data points can be a hurdle in data analysis. Removing rows or columns with these blanks may seem like a quick fix, but it can discard valuable information. Thankfully, there are other options! Data imputation allows you to estimate missing values. This can involve replacing them with averages, using sophisticated algorithms, or even creating a special category for “missing” data itself. The most effective method depends on the type of data you’re working with (numbers or categories) and the reason the data is missing in the first place.\\n\\n⌘\\xa0Spotting and Managing Outliers\\nOutliers, data points that stray far from the pack, can wreak havoc on your analysis. To identify them, keep an eye on visualizations like boxplots. Once spotted, you have choices. Clear errors can be removed, but be mindful as outliers can sometimes reveal hidden truths! Consider investigating the cause, it might expose a fascinating trend. Alternatively, you can replace the outlier with a more fitting value, like the median. The best course of action depends on your specific data and analysis goals.\\n\\n⌘\\xa0Eliminating Duplicates\\nDuplicate data can make your analysis a messy affair. Fortunately, eliminating them is a straightforward process. Most spreadsheet programs offer a “remove duplicates” function. This handy tool identifies and removes exact copies within your chosen data set, streamlining your analysis. However, be aware that slight variations in duplicates might slip through the cracks. To ensure a truly clean dataset, you may need to specify which columns to compare for complete elimination.\\n\\n⌘\\xa0Transforming Texts for Clarity\\nText data cleaning often involves wrestling with inconsistencies and unwanted characters. Here’s where regular expressions (regex) come in as your secret weapon. Regex uses patterns to identify and manipulate text. You can write a regex pattern to target things like extra spaces, punctuation, special symbols, or even hashtags. Once you’ve identified the pattern, you can replace it with a clean alternative, like an empty space or complete removal. This wrangles your text data into a uniform format, making it easier to analyze and interpret for better results.\\n\\n⌘\\xa0Taming Categorical Variables\\nFor many machine learning tasks, categorical variables need to be transformed into numerical values. One approach is encoding, where we simply assign a unique number to each category. However, for some situations, we might want to encode in a way that reflects the order or relationship between the categories. This can be achieved through techniques like one-hot encoding, which creates a new binary variable for each category.\\n\\n⌘\\xa0Date & Time Mastery\\nData from different sources might use YYYY-MM-DD, MM/DD/YYYY, or even cryptic abbreviations. Standardization tools can convert everything to a single, consistent format. Advanced cleaning might involve handling partial dates (like just a year) or timestamps with time zones. Here, we might need to decide on a default date or time zone to ensure consistency. Finally, some analyses might require extracting specific parts from the date and time data, like the day of the week or hour.'),\n",
       "  Document(page_content='⌘ Solving Imbalanced Data Distribution\\nMore than standard techniques might be needed for highly imbalanced data, where one class vastly outnumbers the others. Here’s where advanced approaches come in. Resampling methods like SMOTE (Synthetic Minority Oversampling Technique) can create synthetic data points for the minority class. Imagine generating new, realistic data examples to balance the scales! Additionally, specialized algorithms are designed specifically for imbalanced data. These algorithms can place more weight on the minority class during training, ensuring the model pays closer attention to the rarer but crucial examples.\\n\\n⌘ Data\\xa0Scaling for Calibration\\nFeatures in your data can have uneven scales. Data scaling fixes this! Normalization transforms all features to have a mean of zero and a standard deviation of one, ensuring each feature contributes equally. Min-max scaling, on the other hand, rescales features to a specific range, often 0 to 1. By applying these scaling techniques, you create a level playing field for all features in your data.\\n\\n⌘ Adjusting for Data Skewness\\nEncountering skewed data, where values favor one extreme, can throw your analysis off balance. Imagine analyzing income with a ruler that only shows low numbers! To tackle this, data transformations come in handy. A popular technique is the log transformation, which acts like a data shrink ray. It compresses large values, making them less influential and giving smaller values a fairer chance.\\n\\n⌘\\xa0Mapping the Data Journey\\nData lineage tracks the entire journey of a data point, like a detailed travel log. It shows where the data came from, all its transformations, and its final use in analysis or reports. This transparency helps identify bottlenecks, ensure data quality, and meet regulations.\\nData provenance, on the other hand, focuses on the data’s origin story. It’s like a birth certificate, establishing the source and any inherent qualities the data might have. Knowing the source helps assess its credibility and allows for reproducing analyses or debugging errors. By tracking lineage and provenance, organizations can ensure data quality and facilitate debugging if issues arise.\\n\\nConclusion\\nI hope you enjoyed reading this article on important data-cleaning concepts we should follow as a checklist. If you would like to connect with me to discuss this topic further, please reach out via LinkedIn or email me directly. I’m always happy to answer any additional questions you may have on statistics or other data science techniques.\\nThank you for reading, and I look forward to hearing from you!')],\n",
       " 'output_text': ' \\n\\nThis article highlights the significance of data cleaning in accurate data analysis. It covers advanced techniques for dealing with missing values, outliers, duplicates, and transforming text and categorical data. The author also discusses methods for managing date and time data and extracting specific information. Additionally, it examines strategies for addressing imbalanced data distribution, skewness, and lineage and provenance tracking. The author encourages further discussion on these topics through LinkedIn or email.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.invoke(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593072c",
   "metadata": {},
   "source": [
    "## HTTP Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320a57a5",
   "metadata": {},
   "source": [
    "### LLMRequestsChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a29e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMRequestsChain, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f23480",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Extract the answer to the question '{query}' or say \"not found\" if the information is not available.\n",
    "{requests_result}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"requests_result\"],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5dc9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd539d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMRequestsChain(llm_chain=LLMChain(llm=llm, prompt=PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3fbc27",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Preparing the question & inputs to the http request<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64893b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the capital of india?\"\n",
    "inputs = {\n",
    "    \"query\": question,\n",
    "    \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "779f0411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the capital of india?',\n",
       " 'url': 'https://www.google.com/search?q=What+is+the+capital+of+india?',\n",
       " 'output': '\\nNew Delhi'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c980e2",
   "metadata": {},
   "source": [
    "<font color='green'>\n",
    "Let's look at the internal functioning<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b974d187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, Any],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, Any]:\n",
      "        from bs4 import BeautifulSoup\n",
      "\n",
      "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
      "        # Other keys are assumed to be needed for LLM prediction\n",
      "        other_keys = {k: v for k, v in inputs.items() if k != self.input_key}\n",
      "        url = inputs[self.input_key]\n",
      "        res = self.requests_wrapper.get(url)\n",
      "        # extract the text from the html\n",
      "        soup = BeautifulSoup(res, \"html.parser\")\n",
      "        other_keys[self.requests_key] = soup.get_text()[: self.text_length]\n",
      "        result = self.llm_chain.predict(\n",
      "            callbacks=_run_manager.get_child(), **other_keys\n",
      "        )\n",
      "        return {self.output_key: result}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.getsource(chain._call))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
